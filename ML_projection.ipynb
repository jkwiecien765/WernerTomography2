{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from myPackage.my_module import *\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10000 done\n",
      "20/10000 done\n",
      "30/10000 done\n",
      "40/10000 done\n",
      "50/10000 done\n",
      "60/10000 done\n",
      "70/10000 done\n",
      "80/10000 done\n",
      "90/10000 done\n",
      "100/10000 done\n",
      "110/10000 done\n",
      "120/10000 done\n",
      "130/10000 done\n",
      "140/10000 done\n",
      "150/10000 done\n",
      "160/10000 done\n",
      "170/10000 done\n",
      "180/10000 done\n",
      "190/10000 done\n",
      "200/10000 done\n",
      "210/10000 done\n",
      "220/10000 done\n",
      "230/10000 done\n",
      "240/10000 done\n",
      "250/10000 done\n",
      "260/10000 done\n",
      "270/10000 done\n",
      "280/10000 done\n",
      "290/10000 done\n",
      "300/10000 done\n",
      "310/10000 done\n",
      "320/10000 done\n",
      "330/10000 done\n",
      "340/10000 done\n",
      "350/10000 done\n",
      "360/10000 done\n",
      "370/10000 done\n",
      "380/10000 done\n",
      "390/10000 done\n",
      "400/10000 done\n",
      "410/10000 done\n",
      "420/10000 done\n",
      "430/10000 done\n",
      "440/10000 done\n",
      "450/10000 done\n",
      "460/10000 done\n",
      "470/10000 done\n",
      "480/10000 done\n",
      "490/10000 done\n",
      "500/10000 done\n",
      "510/10000 done\n",
      "520/10000 done\n",
      "530/10000 done\n",
      "540/10000 done\n",
      "550/10000 done\n",
      "560/10000 done\n",
      "570/10000 done\n",
      "580/10000 done\n",
      "590/10000 done\n",
      "600/10000 done\n",
      "610/10000 done\n",
      "620/10000 done\n",
      "630/10000 done\n",
      "640/10000 done\n",
      "650/10000 done\n",
      "660/10000 done\n",
      "670/10000 done\n",
      "680/10000 done\n",
      "690/10000 done\n",
      "700/10000 done\n",
      "710/10000 done\n",
      "720/10000 done\n",
      "730/10000 done\n",
      "740/10000 done\n",
      "750/10000 done\n",
      "760/10000 done\n",
      "770/10000 done\n",
      "780/10000 done\n",
      "790/10000 done\n",
      "800/10000 done\n",
      "810/10000 done\n",
      "820/10000 done\n",
      "830/10000 done\n",
      "840/10000 done\n",
      "850/10000 done\n",
      "860/10000 done\n",
      "870/10000 done\n",
      "880/10000 done\n",
      "890/10000 done\n",
      "900/10000 done\n",
      "910/10000 done\n",
      "920/10000 done\n",
      "930/10000 done\n",
      "940/10000 done\n",
      "950/10000 done\n",
      "960/10000 done\n",
      "970/10000 done\n",
      "980/10000 done\n",
      "990/10000 done\n",
      "1000/10000 done\n",
      "1010/10000 done\n",
      "1020/10000 done\n",
      "1030/10000 done\n",
      "1040/10000 done\n",
      "1050/10000 done\n",
      "1060/10000 done\n",
      "1070/10000 done\n",
      "1080/10000 done\n",
      "1090/10000 done\n",
      "1100/10000 done\n",
      "1110/10000 done\n",
      "1120/10000 done\n",
      "1130/10000 done\n",
      "1140/10000 done\n",
      "1150/10000 done\n",
      "1160/10000 done\n",
      "1170/10000 done\n",
      "1180/10000 done\n",
      "1190/10000 done\n",
      "1200/10000 done\n",
      "1210/10000 done\n",
      "1220/10000 done\n",
      "1230/10000 done\n",
      "1240/10000 done\n",
      "1250/10000 done\n",
      "1260/10000 done\n",
      "1270/10000 done\n",
      "1280/10000 done\n",
      "1290/10000 done\n",
      "1300/10000 done\n",
      "1310/10000 done\n",
      "1320/10000 done\n",
      "1330/10000 done\n",
      "1340/10000 done\n",
      "1350/10000 done\n",
      "1360/10000 done\n",
      "1370/10000 done\n",
      "1380/10000 done\n",
      "1390/10000 done\n",
      "1400/10000 done\n",
      "1410/10000 done\n",
      "1420/10000 done\n",
      "1430/10000 done\n",
      "1440/10000 done\n",
      "1450/10000 done\n",
      "1460/10000 done\n",
      "1470/10000 done\n",
      "1480/10000 done\n",
      "1490/10000 done\n",
      "1500/10000 done\n",
      "1510/10000 done\n",
      "1520/10000 done\n",
      "1530/10000 done\n",
      "1540/10000 done\n",
      "1550/10000 done\n",
      "1560/10000 done\n",
      "1570/10000 done\n",
      "1580/10000 done\n",
      "1590/10000 done\n",
      "1600/10000 done\n",
      "1610/10000 done\n",
      "1620/10000 done\n",
      "1630/10000 done\n",
      "1640/10000 done\n",
      "1650/10000 done\n",
      "1660/10000 done\n",
      "1670/10000 done\n",
      "1680/10000 done\n",
      "1690/10000 done\n",
      "1700/10000 done\n",
      "1710/10000 done\n",
      "1720/10000 done\n",
      "1730/10000 done\n",
      "1740/10000 done\n",
      "1750/10000 done\n",
      "1760/10000 done\n",
      "1770/10000 done\n",
      "1780/10000 done\n",
      "1790/10000 done\n",
      "1800/10000 done\n",
      "1810/10000 done\n",
      "1820/10000 done\n",
      "1830/10000 done\n",
      "1840/10000 done\n",
      "1850/10000 done\n",
      "1860/10000 done\n",
      "1870/10000 done\n",
      "1880/10000 done\n",
      "1890/10000 done\n",
      "1900/10000 done\n",
      "1910/10000 done\n",
      "1920/10000 done\n",
      "1930/10000 done\n",
      "1940/10000 done\n",
      "1950/10000 done\n",
      "1960/10000 done\n",
      "1970/10000 done\n",
      "1980/10000 done\n",
      "1990/10000 done\n",
      "2000/10000 done\n",
      "2010/10000 done\n",
      "2020/10000 done\n",
      "2030/10000 done\n",
      "2040/10000 done\n",
      "2050/10000 done\n",
      "2060/10000 done\n",
      "2070/10000 done\n",
      "2080/10000 done\n",
      "2090/10000 done\n",
      "2100/10000 done\n",
      "2110/10000 done\n",
      "2120/10000 done\n",
      "2130/10000 done\n",
      "2140/10000 done\n",
      "2150/10000 done\n",
      "2160/10000 done\n",
      "2170/10000 done\n",
      "2180/10000 done\n",
      "2190/10000 done\n",
      "2200/10000 done\n",
      "2210/10000 done\n",
      "2220/10000 done\n",
      "2230/10000 done\n",
      "2240/10000 done\n",
      "2250/10000 done\n",
      "2260/10000 done\n",
      "2270/10000 done\n",
      "2280/10000 done\n",
      "2290/10000 done\n",
      "2300/10000 done\n",
      "2310/10000 done\n",
      "2320/10000 done\n",
      "2330/10000 done\n",
      "2340/10000 done\n",
      "2350/10000 done\n",
      "2360/10000 done\n",
      "2370/10000 done\n",
      "2380/10000 done\n",
      "2390/10000 done\n",
      "2400/10000 done\n",
      "2410/10000 done\n",
      "2420/10000 done\n",
      "2430/10000 done\n",
      "2440/10000 done\n",
      "2450/10000 done\n",
      "2460/10000 done\n",
      "2470/10000 done\n",
      "2480/10000 done\n",
      "2490/10000 done\n",
      "2500/10000 done\n",
      "2510/10000 done\n",
      "2520/10000 done\n",
      "2530/10000 done\n",
      "2540/10000 done\n",
      "2550/10000 done\n",
      "2560/10000 done\n",
      "2570/10000 done\n",
      "2580/10000 done\n",
      "2590/10000 done\n",
      "2600/10000 done\n",
      "2610/10000 done\n",
      "2620/10000 done\n",
      "2630/10000 done\n",
      "2640/10000 done\n",
      "2650/10000 done\n",
      "2660/10000 done\n",
      "2670/10000 done\n",
      "2680/10000 done\n",
      "2690/10000 done\n",
      "2700/10000 done\n",
      "2710/10000 done\n",
      "2720/10000 done\n",
      "2730/10000 done\n",
      "2740/10000 done\n",
      "2750/10000 done\n",
      "2760/10000 done\n",
      "2770/10000 done\n",
      "2780/10000 done\n",
      "2790/10000 done\n",
      "2800/10000 done\n",
      "2810/10000 done\n",
      "2820/10000 done\n",
      "2830/10000 done\n",
      "2840/10000 done\n",
      "2850/10000 done\n",
      "2860/10000 done\n",
      "2870/10000 done\n",
      "2880/10000 done\n",
      "2890/10000 done\n",
      "2900/10000 done\n",
      "2910/10000 done\n",
      "2920/10000 done\n",
      "2930/10000 done\n",
      "2940/10000 done\n",
      "2950/10000 done\n",
      "2960/10000 done\n",
      "2970/10000 done\n",
      "2980/10000 done\n",
      "2990/10000 done\n",
      "3000/10000 done\n",
      "3010/10000 done\n",
      "3020/10000 done\n",
      "3030/10000 done\n",
      "3040/10000 done\n",
      "3050/10000 done\n",
      "3060/10000 done\n",
      "3070/10000 done\n",
      "3080/10000 done\n",
      "3090/10000 done\n",
      "3100/10000 done\n",
      "3110/10000 done\n",
      "3120/10000 done\n",
      "3130/10000 done\n",
      "3140/10000 done\n",
      "3150/10000 done\n",
      "3160/10000 done\n",
      "3170/10000 done\n",
      "3180/10000 done\n",
      "3190/10000 done\n",
      "3200/10000 done\n",
      "3210/10000 done\n",
      "3220/10000 done\n",
      "3230/10000 done\n",
      "3240/10000 done\n",
      "3250/10000 done\n",
      "3260/10000 done\n",
      "3270/10000 done\n",
      "3280/10000 done\n",
      "3290/10000 done\n",
      "3300/10000 done\n",
      "3310/10000 done\n",
      "3320/10000 done\n",
      "3330/10000 done\n",
      "3340/10000 done\n",
      "3350/10000 done\n",
      "3360/10000 done\n",
      "3370/10000 done\n",
      "3380/10000 done\n",
      "3390/10000 done\n",
      "3400/10000 done\n",
      "3410/10000 done\n",
      "3420/10000 done\n",
      "3430/10000 done\n",
      "3440/10000 done\n",
      "3450/10000 done\n",
      "3460/10000 done\n",
      "3470/10000 done\n",
      "3480/10000 done\n",
      "3490/10000 done\n",
      "3500/10000 done\n",
      "3510/10000 done\n",
      "3520/10000 done\n",
      "3530/10000 done\n",
      "3540/10000 done\n",
      "3550/10000 done\n",
      "3560/10000 done\n",
      "3570/10000 done\n",
      "3580/10000 done\n",
      "3590/10000 done\n",
      "3600/10000 done\n",
      "3610/10000 done\n",
      "3620/10000 done\n",
      "3630/10000 done\n",
      "3640/10000 done\n",
      "3650/10000 done\n",
      "3660/10000 done\n",
      "3670/10000 done\n",
      "3680/10000 done\n",
      "3690/10000 done\n",
      "3700/10000 done\n",
      "3710/10000 done\n",
      "3720/10000 done\n",
      "3730/10000 done\n",
      "3740/10000 done\n",
      "3750/10000 done\n",
      "3760/10000 done\n",
      "3770/10000 done\n",
      "3780/10000 done\n",
      "3790/10000 done\n",
      "3800/10000 done\n",
      "3810/10000 done\n",
      "3820/10000 done\n",
      "3830/10000 done\n",
      "3840/10000 done\n",
      "3850/10000 done\n",
      "3860/10000 done\n",
      "3870/10000 done\n",
      "3880/10000 done\n",
      "3890/10000 done\n",
      "3900/10000 done\n",
      "3910/10000 done\n",
      "3920/10000 done\n",
      "3930/10000 done\n",
      "3940/10000 done\n",
      "3950/10000 done\n",
      "3960/10000 done\n",
      "3970/10000 done\n",
      "3980/10000 done\n",
      "3990/10000 done\n",
      "4000/10000 done\n",
      "4010/10000 done\n",
      "4020/10000 done\n",
      "4030/10000 done\n",
      "4040/10000 done\n",
      "4050/10000 done\n",
      "4060/10000 done\n",
      "4070/10000 done\n",
      "4080/10000 done\n",
      "4090/10000 done\n",
      "4100/10000 done\n",
      "4110/10000 done\n",
      "4120/10000 done\n",
      "4130/10000 done\n",
      "4140/10000 done\n",
      "4150/10000 done\n",
      "4160/10000 done\n",
      "4170/10000 done\n",
      "4180/10000 done\n",
      "4190/10000 done\n",
      "4200/10000 done\n",
      "4210/10000 done\n",
      "4220/10000 done\n",
      "4230/10000 done\n",
      "4240/10000 done\n",
      "4250/10000 done\n",
      "4260/10000 done\n",
      "4270/10000 done\n",
      "4280/10000 done\n",
      "4290/10000 done\n",
      "4300/10000 done\n",
      "4310/10000 done\n",
      "4320/10000 done\n",
      "4330/10000 done\n",
      "4340/10000 done\n",
      "4350/10000 done\n",
      "4360/10000 done\n",
      "4370/10000 done\n",
      "4380/10000 done\n",
      "4390/10000 done\n",
      "4400/10000 done\n",
      "4410/10000 done\n",
      "4420/10000 done\n",
      "4430/10000 done\n",
      "4440/10000 done\n",
      "4450/10000 done\n",
      "4460/10000 done\n",
      "4470/10000 done\n",
      "4480/10000 done\n",
      "4490/10000 done\n",
      "4500/10000 done\n",
      "4510/10000 done\n",
      "4520/10000 done\n",
      "4530/10000 done\n",
      "4540/10000 done\n",
      "4550/10000 done\n",
      "4560/10000 done\n",
      "4570/10000 done\n",
      "4580/10000 done\n",
      "4590/10000 done\n",
      "4600/10000 done\n",
      "4610/10000 done\n",
      "4620/10000 done\n",
      "4630/10000 done\n",
      "4640/10000 done\n",
      "4650/10000 done\n",
      "4660/10000 done\n",
      "4670/10000 done\n",
      "4680/10000 done\n",
      "4690/10000 done\n",
      "4700/10000 done\n",
      "4710/10000 done\n",
      "4720/10000 done\n",
      "4730/10000 done\n",
      "4740/10000 done\n",
      "4750/10000 done\n",
      "4760/10000 done\n",
      "4770/10000 done\n",
      "4780/10000 done\n",
      "4790/10000 done\n",
      "4800/10000 done\n",
      "4810/10000 done\n",
      "4820/10000 done\n",
      "4830/10000 done\n",
      "4840/10000 done\n",
      "4850/10000 done\n",
      "4860/10000 done\n",
      "4870/10000 done\n",
      "4880/10000 done\n",
      "4890/10000 done\n",
      "4900/10000 done\n",
      "4910/10000 done\n",
      "4920/10000 done\n",
      "4930/10000 done\n",
      "4940/10000 done\n",
      "4950/10000 done\n",
      "4960/10000 done\n",
      "4970/10000 done\n",
      "4980/10000 done\n",
      "4990/10000 done\n",
      "5000/10000 done\n",
      "5010/10000 done\n",
      "5020/10000 done\n",
      "5030/10000 done\n",
      "5040/10000 done\n",
      "5050/10000 done\n",
      "5060/10000 done\n",
      "5070/10000 done\n",
      "5080/10000 done\n",
      "5090/10000 done\n",
      "5100/10000 done\n",
      "5110/10000 done\n",
      "5120/10000 done\n",
      "5130/10000 done\n",
      "5140/10000 done\n",
      "5150/10000 done\n",
      "5160/10000 done\n",
      "5170/10000 done\n",
      "5180/10000 done\n",
      "5190/10000 done\n",
      "5200/10000 done\n",
      "5210/10000 done\n",
      "5220/10000 done\n",
      "5230/10000 done\n",
      "5240/10000 done\n",
      "5250/10000 done\n",
      "5260/10000 done\n",
      "5270/10000 done\n",
      "5280/10000 done\n",
      "5290/10000 done\n",
      "5300/10000 done\n",
      "5310/10000 done\n",
      "5320/10000 done\n",
      "5330/10000 done\n",
      "5340/10000 done\n",
      "5350/10000 done\n",
      "5360/10000 done\n",
      "5370/10000 done\n",
      "5380/10000 done\n",
      "5390/10000 done\n",
      "5400/10000 done\n",
      "5410/10000 done\n",
      "5420/10000 done\n",
      "5430/10000 done\n",
      "5440/10000 done\n",
      "5450/10000 done\n",
      "5460/10000 done\n",
      "5470/10000 done\n",
      "5480/10000 done\n",
      "5490/10000 done\n",
      "5500/10000 done\n",
      "5510/10000 done\n",
      "5520/10000 done\n",
      "5530/10000 done\n",
      "5540/10000 done\n",
      "5550/10000 done\n",
      "5560/10000 done\n",
      "5570/10000 done\n",
      "5580/10000 done\n",
      "5590/10000 done\n",
      "5600/10000 done\n",
      "5610/10000 done\n",
      "5620/10000 done\n",
      "5630/10000 done\n",
      "5640/10000 done\n",
      "5650/10000 done\n",
      "5660/10000 done\n",
      "5670/10000 done\n",
      "5680/10000 done\n",
      "5690/10000 done\n",
      "5700/10000 done\n",
      "5710/10000 done\n",
      "5720/10000 done\n",
      "5730/10000 done\n",
      "5740/10000 done\n",
      "5750/10000 done\n",
      "5760/10000 done\n",
      "5770/10000 done\n",
      "5780/10000 done\n",
      "5790/10000 done\n",
      "5800/10000 done\n",
      "5810/10000 done\n",
      "5820/10000 done\n",
      "5830/10000 done\n",
      "5840/10000 done\n",
      "5850/10000 done\n",
      "5860/10000 done\n",
      "5870/10000 done\n",
      "5880/10000 done\n",
      "5890/10000 done\n",
      "5900/10000 done\n",
      "5910/10000 done\n",
      "5920/10000 done\n",
      "5930/10000 done\n",
      "5940/10000 done\n",
      "5950/10000 done\n",
      "5960/10000 done\n",
      "5970/10000 done\n",
      "5980/10000 done\n",
      "5990/10000 done\n",
      "6000/10000 done\n",
      "6010/10000 done\n",
      "6020/10000 done\n",
      "6030/10000 done\n",
      "6040/10000 done\n",
      "6050/10000 done\n",
      "6060/10000 done\n",
      "6070/10000 done\n",
      "6080/10000 done\n",
      "6090/10000 done\n",
      "6100/10000 done\n",
      "6110/10000 done\n",
      "6120/10000 done\n",
      "6130/10000 done\n",
      "6140/10000 done\n",
      "6150/10000 done\n",
      "6160/10000 done\n",
      "6170/10000 done\n",
      "6180/10000 done\n",
      "6190/10000 done\n",
      "6200/10000 done\n",
      "6210/10000 done\n",
      "6220/10000 done\n",
      "6230/10000 done\n",
      "6240/10000 done\n",
      "6250/10000 done\n",
      "6260/10000 done\n",
      "6270/10000 done\n",
      "6280/10000 done\n",
      "6290/10000 done\n",
      "6300/10000 done\n",
      "6310/10000 done\n",
      "6320/10000 done\n",
      "6330/10000 done\n",
      "6340/10000 done\n",
      "6350/10000 done\n",
      "6360/10000 done\n",
      "6370/10000 done\n",
      "6380/10000 done\n",
      "6390/10000 done\n",
      "6400/10000 done\n",
      "6410/10000 done\n",
      "6420/10000 done\n",
      "6430/10000 done\n",
      "6440/10000 done\n",
      "6450/10000 done\n",
      "6460/10000 done\n",
      "6470/10000 done\n",
      "6480/10000 done\n",
      "6490/10000 done\n",
      "6500/10000 done\n",
      "6510/10000 done\n",
      "6520/10000 done\n",
      "6530/10000 done\n",
      "6540/10000 done\n",
      "6550/10000 done\n",
      "6560/10000 done\n",
      "6570/10000 done\n",
      "6580/10000 done\n",
      "6590/10000 done\n",
      "6600/10000 done\n",
      "6610/10000 done\n",
      "6620/10000 done\n",
      "6630/10000 done\n",
      "6640/10000 done\n",
      "6650/10000 done\n",
      "6660/10000 done\n",
      "6670/10000 done\n",
      "6680/10000 done\n",
      "6690/10000 done\n",
      "6700/10000 done\n",
      "6710/10000 done\n",
      "6720/10000 done\n",
      "6730/10000 done\n",
      "6740/10000 done\n",
      "6750/10000 done\n",
      "6760/10000 done\n",
      "6770/10000 done\n",
      "6780/10000 done\n",
      "6790/10000 done\n",
      "6800/10000 done\n",
      "6810/10000 done\n",
      "6820/10000 done\n",
      "6830/10000 done\n",
      "6840/10000 done\n",
      "6850/10000 done\n",
      "6860/10000 done\n",
      "6870/10000 done\n",
      "6880/10000 done\n",
      "6890/10000 done\n",
      "6900/10000 done\n",
      "6910/10000 done\n",
      "6920/10000 done\n",
      "6930/10000 done\n",
      "6940/10000 done\n",
      "6950/10000 done\n",
      "6960/10000 done\n",
      "6970/10000 done\n",
      "6980/10000 done\n",
      "6990/10000 done\n",
      "7000/10000 done\n",
      "7010/10000 done\n",
      "7020/10000 done\n",
      "7030/10000 done\n",
      "7040/10000 done\n",
      "7050/10000 done\n",
      "7060/10000 done\n",
      "7070/10000 done\n",
      "7080/10000 done\n",
      "7090/10000 done\n",
      "7100/10000 done\n",
      "7110/10000 done\n",
      "7120/10000 done\n",
      "7130/10000 done\n",
      "7140/10000 done\n",
      "7150/10000 done\n",
      "7160/10000 done\n",
      "7170/10000 done\n",
      "7180/10000 done\n",
      "7190/10000 done\n",
      "7200/10000 done\n",
      "7210/10000 done\n",
      "7220/10000 done\n",
      "7230/10000 done\n",
      "7240/10000 done\n",
      "7250/10000 done\n",
      "7260/10000 done\n",
      "7270/10000 done\n",
      "7280/10000 done\n",
      "7290/10000 done\n",
      "7300/10000 done\n",
      "7310/10000 done\n",
      "7320/10000 done\n",
      "7330/10000 done\n",
      "7340/10000 done\n",
      "7350/10000 done\n",
      "7360/10000 done\n",
      "7370/10000 done\n",
      "7380/10000 done\n",
      "7390/10000 done\n",
      "7400/10000 done\n",
      "7410/10000 done\n",
      "7420/10000 done\n",
      "7430/10000 done\n",
      "7440/10000 done\n",
      "7450/10000 done\n",
      "7460/10000 done\n",
      "7470/10000 done\n",
      "7480/10000 done\n",
      "7490/10000 done\n",
      "7500/10000 done\n",
      "7510/10000 done\n",
      "7520/10000 done\n",
      "7530/10000 done\n",
      "7540/10000 done\n",
      "7550/10000 done\n",
      "7560/10000 done\n",
      "7570/10000 done\n",
      "7580/10000 done\n",
      "7590/10000 done\n",
      "7600/10000 done\n",
      "7610/10000 done\n",
      "7620/10000 done\n",
      "7630/10000 done\n",
      "7640/10000 done\n",
      "7650/10000 done\n",
      "7660/10000 done\n",
      "7670/10000 done\n",
      "7680/10000 done\n",
      "7690/10000 done\n",
      "7700/10000 done\n",
      "7710/10000 done\n",
      "7720/10000 done\n",
      "7730/10000 done\n",
      "7740/10000 done\n",
      "7750/10000 done\n",
      "7760/10000 done\n",
      "7770/10000 done\n",
      "7780/10000 done\n",
      "7790/10000 done\n",
      "7800/10000 done\n",
      "7810/10000 done\n",
      "7820/10000 done\n",
      "7830/10000 done\n",
      "7840/10000 done\n",
      "7850/10000 done\n",
      "7860/10000 done\n",
      "7870/10000 done\n",
      "7880/10000 done\n",
      "7890/10000 done\n",
      "7900/10000 done\n",
      "7910/10000 done\n",
      "7920/10000 done\n",
      "7930/10000 done\n",
      "7940/10000 done\n",
      "7950/10000 done\n",
      "7960/10000 done\n",
      "7970/10000 done\n",
      "7980/10000 done\n",
      "7990/10000 done\n",
      "8000/10000 done\n",
      "8010/10000 done\n",
      "8020/10000 done\n",
      "8030/10000 done\n",
      "8040/10000 done\n",
      "8050/10000 done\n",
      "8060/10000 done\n",
      "8070/10000 done\n",
      "8080/10000 done\n",
      "8090/10000 done\n",
      "8100/10000 done\n",
      "8110/10000 done\n",
      "8120/10000 done\n",
      "8130/10000 done\n",
      "8140/10000 done\n",
      "8150/10000 done\n",
      "8160/10000 done\n",
      "8170/10000 done\n",
      "8180/10000 done\n",
      "8190/10000 done\n",
      "8200/10000 done\n",
      "8210/10000 done\n",
      "8220/10000 done\n",
      "8230/10000 done\n",
      "8240/10000 done\n",
      "8250/10000 done\n",
      "8260/10000 done\n",
      "8270/10000 done\n",
      "8280/10000 done\n",
      "8290/10000 done\n",
      "8300/10000 done\n",
      "8310/10000 done\n",
      "8320/10000 done\n",
      "8330/10000 done\n",
      "8340/10000 done\n",
      "8350/10000 done\n",
      "8360/10000 done\n",
      "8370/10000 done\n",
      "8380/10000 done\n",
      "8390/10000 done\n",
      "8400/10000 done\n",
      "8410/10000 done\n",
      "8420/10000 done\n",
      "8430/10000 done\n",
      "8440/10000 done\n",
      "8450/10000 done\n",
      "8460/10000 done\n",
      "8470/10000 done\n",
      "8480/10000 done\n",
      "8490/10000 done\n",
      "8500/10000 done\n",
      "8510/10000 done\n",
      "8520/10000 done\n",
      "8530/10000 done\n",
      "8540/10000 done\n",
      "8550/10000 done\n",
      "8560/10000 done\n",
      "8570/10000 done\n",
      "8580/10000 done\n",
      "8590/10000 done\n",
      "8600/10000 done\n",
      "8610/10000 done\n",
      "8620/10000 done\n",
      "8630/10000 done\n",
      "8640/10000 done\n",
      "8650/10000 done\n",
      "8660/10000 done\n",
      "8670/10000 done\n",
      "8680/10000 done\n",
      "8690/10000 done\n",
      "8700/10000 done\n",
      "8710/10000 done\n",
      "8720/10000 done\n",
      "8730/10000 done\n",
      "8740/10000 done\n",
      "8750/10000 done\n",
      "8760/10000 done\n",
      "8770/10000 done\n",
      "8780/10000 done\n",
      "8790/10000 done\n",
      "8800/10000 done\n",
      "8810/10000 done\n",
      "8820/10000 done\n",
      "8830/10000 done\n",
      "8840/10000 done\n",
      "8850/10000 done\n",
      "8860/10000 done\n",
      "8870/10000 done\n",
      "8880/10000 done\n",
      "8890/10000 done\n",
      "8900/10000 done\n",
      "8910/10000 done\n",
      "8920/10000 done\n",
      "8930/10000 done\n",
      "8940/10000 done\n",
      "8950/10000 done\n",
      "8960/10000 done\n",
      "8970/10000 done\n",
      "8980/10000 done\n",
      "8990/10000 done\n",
      "9000/10000 done\n",
      "9010/10000 done\n",
      "9020/10000 done\n",
      "9030/10000 done\n",
      "9040/10000 done\n",
      "9050/10000 done\n",
      "9060/10000 done\n",
      "9070/10000 done\n",
      "9080/10000 done\n",
      "9090/10000 done\n",
      "9100/10000 done\n",
      "9110/10000 done\n",
      "9120/10000 done\n",
      "9130/10000 done\n",
      "9140/10000 done\n",
      "9150/10000 done\n",
      "9160/10000 done\n",
      "9170/10000 done\n",
      "9180/10000 done\n",
      "9190/10000 done\n",
      "9200/10000 done\n",
      "9210/10000 done\n",
      "9220/10000 done\n",
      "9230/10000 done\n",
      "9240/10000 done\n",
      "9250/10000 done\n",
      "9260/10000 done\n",
      "9270/10000 done\n",
      "9280/10000 done\n",
      "9290/10000 done\n",
      "9300/10000 done\n",
      "9310/10000 done\n",
      "9320/10000 done\n",
      "9330/10000 done\n",
      "9340/10000 done\n",
      "9350/10000 done\n",
      "9360/10000 done\n",
      "9370/10000 done\n",
      "9380/10000 done\n",
      "9390/10000 done\n",
      "9400/10000 done\n",
      "9410/10000 done\n",
      "9420/10000 done\n",
      "9430/10000 done\n",
      "9440/10000 done\n",
      "9450/10000 done\n",
      "9460/10000 done\n",
      "9470/10000 done\n",
      "9480/10000 done\n",
      "9490/10000 done\n",
      "9500/10000 done\n",
      "9510/10000 done\n",
      "9520/10000 done\n",
      "9530/10000 done\n",
      "9540/10000 done\n",
      "9550/10000 done\n",
      "9560/10000 done\n",
      "9570/10000 done\n",
      "9580/10000 done\n",
      "9590/10000 done\n",
      "9600/10000 done\n",
      "9610/10000 done\n",
      "9620/10000 done\n",
      "9630/10000 done\n",
      "9640/10000 done\n",
      "9650/10000 done\n",
      "9660/10000 done\n",
      "9670/10000 done\n",
      "9680/10000 done\n",
      "9690/10000 done\n",
      "9700/10000 done\n",
      "9710/10000 done\n",
      "9720/10000 done\n",
      "9730/10000 done\n",
      "9740/10000 done\n",
      "9750/10000 done\n",
      "9760/10000 done\n",
      "9770/10000 done\n",
      "9780/10000 done\n",
      "9790/10000 done\n",
      "9800/10000 done\n",
      "9810/10000 done\n",
      "9820/10000 done\n",
      "9830/10000 done\n",
      "9840/10000 done\n",
      "9850/10000 done\n",
      "9860/10000 done\n",
      "9870/10000 done\n",
      "9880/10000 done\n",
      "9890/10000 done\n",
      "9900/10000 done\n",
      "9910/10000 done\n",
      "9920/10000 done\n",
      "9930/10000 done\n",
      "9940/10000 done\n",
      "9950/10000 done\n",
      "9960/10000 done\n",
      "9970/10000 done\n",
      "9980/10000 done\n",
      "9990/10000 done\n",
      "10000/10000 done\n"
     ]
    }
   ],
   "source": [
    "def data_to_bins(data):\n",
    "    counts=np.zeros(100)\n",
    "    for dat in data:\n",
    "        try:\n",
    "            counts[int(dat*100)]+=1/len(data)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return counts\n",
    "\n",
    "def projection(matrix):\n",
    "    return np.real(np.matrix([[matrix[0,0],0,0,matrix[0,3]/2+matrix[3,0]/2],\\\n",
    "                                [0,matrix[2,2]/2+matrix[1,1]/2,0,0],\\\n",
    "                                [0,0,matrix[2,2]/2+matrix[1,1]/2,0],\\\n",
    "                                [matrix[0,3]/2+matrix[3,0]/2,0,0,matrix[3,3]]]))\n",
    "qubit_switch = np.matrix([[1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]])\n",
    "    \n",
    "def phase_rotation(phi=np.random.random()*2*pi):\n",
    "    from scipy.linalg import expm\n",
    "    sig_z = np.array([[1,0],[0,-1]], dtype='complex')\n",
    "    U = tens_prod2d(expm((1j * phi * sig_z)), expm((-1j * phi * sig_z)))\n",
    "    return U\n",
    "\n",
    "def phase_rotation_dag(phi=np.random.random()*2*pi):\n",
    "    return np.transpose(np.conjugate(phase_rotation(phi)))\n",
    "\n",
    "proj_bins=[]\n",
    "for i in range(10000):\n",
    "    initial_matrix = rand_PSDM(4)\n",
    "    projected = projection(initial_matrix)\n",
    "    parameters = [projected[0,0], projected[0,3], projected[1,1], projected[3,3]]\n",
    "    proj_data=[]\n",
    "    for j in range(2000):\n",
    "        phi=np.random.random()*2*pi\n",
    "        rotated = phase_rotation(phi)@initial_matrix@phase_rotation_dag(phi)\n",
    "        switched = qubit_switch@rotated@qubit_switch\n",
    "        proj_data.append(np.trace(np.real(switched@np.outer(ZeroZero,ZeroZero))))\n",
    "    \n",
    "    proj_bins.append(data_to_bins(proj_data))\n",
    "    \n",
    "    if (i+1)%10 == 0:\n",
    "        print(f'{i+1}/10000 done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24129063356890973]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if len(proj_bins) > 10:\n",
    "    with open('ML_int_proj.dat', 'wb') as file:\n",
    "        pickle.dump(proj_bins, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ML_int_proj.dat', 'rb') as file:\n",
    "    proj_bins_load = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10000 done\n",
      "20/10000 done\n",
      "30/10000 done\n",
      "40/10000 done\n",
      "50/10000 done\n",
      "60/10000 done\n",
      "70/10000 done\n",
      "80/10000 done\n",
      "90/10000 done\n",
      "100/10000 done\n",
      "110/10000 done\n",
      "120/10000 done\n",
      "130/10000 done\n",
      "140/10000 done\n",
      "150/10000 done\n",
      "160/10000 done\n",
      "170/10000 done\n",
      "180/10000 done\n",
      "190/10000 done\n",
      "200/10000 done\n",
      "210/10000 done\n",
      "220/10000 done\n",
      "230/10000 done\n",
      "240/10000 done\n",
      "250/10000 done\n",
      "260/10000 done\n",
      "270/10000 done\n",
      "280/10000 done\n",
      "290/10000 done\n",
      "300/10000 done\n",
      "310/10000 done\n",
      "320/10000 done\n",
      "330/10000 done\n",
      "340/10000 done\n",
      "350/10000 done\n",
      "360/10000 done\n",
      "370/10000 done\n",
      "380/10000 done\n",
      "390/10000 done\n",
      "400/10000 done\n",
      "410/10000 done\n",
      "420/10000 done\n",
      "430/10000 done\n",
      "440/10000 done\n",
      "450/10000 done\n",
      "460/10000 done\n",
      "470/10000 done\n",
      "480/10000 done\n",
      "490/10000 done\n",
      "500/10000 done\n",
      "510/10000 done\n",
      "520/10000 done\n",
      "530/10000 done\n",
      "540/10000 done\n",
      "550/10000 done\n",
      "560/10000 done\n",
      "570/10000 done\n",
      "580/10000 done\n",
      "590/10000 done\n",
      "600/10000 done\n",
      "610/10000 done\n",
      "620/10000 done\n",
      "630/10000 done\n",
      "640/10000 done\n",
      "650/10000 done\n",
      "660/10000 done\n",
      "670/10000 done\n",
      "680/10000 done\n",
      "690/10000 done\n",
      "700/10000 done\n",
      "710/10000 done\n",
      "720/10000 done\n",
      "730/10000 done\n",
      "740/10000 done\n",
      "750/10000 done\n",
      "760/10000 done\n",
      "770/10000 done\n",
      "780/10000 done\n",
      "790/10000 done\n",
      "800/10000 done\n",
      "810/10000 done\n",
      "820/10000 done\n",
      "830/10000 done\n",
      "840/10000 done\n",
      "850/10000 done\n",
      "860/10000 done\n",
      "870/10000 done\n",
      "880/10000 done\n",
      "890/10000 done\n",
      "900/10000 done\n",
      "910/10000 done\n",
      "920/10000 done\n",
      "930/10000 done\n",
      "940/10000 done\n",
      "950/10000 done\n",
      "960/10000 done\n",
      "970/10000 done\n",
      "980/10000 done\n",
      "990/10000 done\n",
      "1000/10000 done\n",
      "1010/10000 done\n",
      "1020/10000 done\n",
      "1030/10000 done\n",
      "1040/10000 done\n",
      "1050/10000 done\n",
      "1060/10000 done\n",
      "1070/10000 done\n",
      "1080/10000 done\n",
      "1090/10000 done\n",
      "1100/10000 done\n",
      "1110/10000 done\n",
      "1120/10000 done\n",
      "1130/10000 done\n",
      "1140/10000 done\n",
      "1150/10000 done\n",
      "1160/10000 done\n",
      "1170/10000 done\n",
      "1180/10000 done\n",
      "1190/10000 done\n",
      "1200/10000 done\n",
      "1210/10000 done\n",
      "1220/10000 done\n",
      "1230/10000 done\n",
      "1240/10000 done\n",
      "1250/10000 done\n",
      "1260/10000 done\n",
      "1270/10000 done\n",
      "1280/10000 done\n",
      "1290/10000 done\n",
      "1300/10000 done\n",
      "1310/10000 done\n",
      "1320/10000 done\n",
      "1330/10000 done\n",
      "1340/10000 done\n",
      "1350/10000 done\n",
      "1360/10000 done\n",
      "1370/10000 done\n",
      "1380/10000 done\n",
      "1390/10000 done\n",
      "1400/10000 done\n",
      "1410/10000 done\n",
      "1420/10000 done\n",
      "1430/10000 done\n",
      "1440/10000 done\n",
      "1450/10000 done\n",
      "1460/10000 done\n",
      "1470/10000 done\n",
      "1480/10000 done\n",
      "1490/10000 done\n",
      "1500/10000 done\n",
      "1510/10000 done\n",
      "1520/10000 done\n",
      "1530/10000 done\n",
      "1540/10000 done\n",
      "1550/10000 done\n",
      "1560/10000 done\n",
      "1570/10000 done\n",
      "1580/10000 done\n",
      "1590/10000 done\n",
      "1600/10000 done\n",
      "1610/10000 done\n",
      "1620/10000 done\n",
      "1630/10000 done\n",
      "1640/10000 done\n",
      "1650/10000 done\n",
      "1660/10000 done\n",
      "1670/10000 done\n",
      "1680/10000 done\n",
      "1690/10000 done\n",
      "1700/10000 done\n",
      "1710/10000 done\n",
      "1720/10000 done\n",
      "1730/10000 done\n",
      "1740/10000 done\n",
      "1750/10000 done\n",
      "1760/10000 done\n",
      "1770/10000 done\n",
      "1780/10000 done\n",
      "1790/10000 done\n",
      "1800/10000 done\n",
      "1810/10000 done\n",
      "1820/10000 done\n",
      "1830/10000 done\n",
      "1840/10000 done\n",
      "1850/10000 done\n",
      "1860/10000 done\n",
      "1870/10000 done\n",
      "1880/10000 done\n",
      "1890/10000 done\n",
      "1900/10000 done\n",
      "1910/10000 done\n",
      "1920/10000 done\n",
      "1930/10000 done\n",
      "1940/10000 done\n",
      "1950/10000 done\n",
      "1960/10000 done\n",
      "1970/10000 done\n",
      "1980/10000 done\n",
      "1990/10000 done\n",
      "2000/10000 done\n",
      "2010/10000 done\n",
      "2020/10000 done\n",
      "2030/10000 done\n",
      "2040/10000 done\n",
      "2050/10000 done\n",
      "2060/10000 done\n",
      "2070/10000 done\n",
      "2080/10000 done\n",
      "2090/10000 done\n",
      "2100/10000 done\n",
      "2110/10000 done\n",
      "2120/10000 done\n",
      "2130/10000 done\n",
      "2140/10000 done\n",
      "2150/10000 done\n",
      "2160/10000 done\n",
      "2170/10000 done\n",
      "2180/10000 done\n",
      "2190/10000 done\n",
      "2200/10000 done\n",
      "2210/10000 done\n",
      "2220/10000 done\n",
      "2230/10000 done\n",
      "2240/10000 done\n",
      "2250/10000 done\n",
      "2260/10000 done\n",
      "2270/10000 done\n",
      "2280/10000 done\n",
      "2290/10000 done\n",
      "2300/10000 done\n",
      "2310/10000 done\n",
      "2320/10000 done\n",
      "2330/10000 done\n",
      "2340/10000 done\n",
      "2350/10000 done\n",
      "2360/10000 done\n",
      "2370/10000 done\n",
      "2380/10000 done\n",
      "2390/10000 done\n",
      "2400/10000 done\n",
      "2410/10000 done\n",
      "2420/10000 done\n",
      "2430/10000 done\n",
      "2440/10000 done\n",
      "2450/10000 done\n",
      "2460/10000 done\n",
      "2470/10000 done\n",
      "2480/10000 done\n",
      "2490/10000 done\n",
      "2500/10000 done\n",
      "2510/10000 done\n",
      "2520/10000 done\n",
      "2530/10000 done\n",
      "2540/10000 done\n",
      "2550/10000 done\n",
      "2560/10000 done\n",
      "2570/10000 done\n",
      "2580/10000 done\n",
      "2590/10000 done\n",
      "2600/10000 done\n",
      "2610/10000 done\n",
      "2620/10000 done\n",
      "2630/10000 done\n",
      "2640/10000 done\n",
      "2650/10000 done\n",
      "2660/10000 done\n",
      "2670/10000 done\n",
      "2680/10000 done\n",
      "2690/10000 done\n",
      "2700/10000 done\n",
      "2710/10000 done\n",
      "2720/10000 done\n",
      "2730/10000 done\n",
      "2740/10000 done\n",
      "2750/10000 done\n",
      "2760/10000 done\n",
      "2770/10000 done\n",
      "2780/10000 done\n",
      "2790/10000 done\n",
      "2800/10000 done\n",
      "2810/10000 done\n",
      "2820/10000 done\n",
      "2830/10000 done\n",
      "2840/10000 done\n",
      "2850/10000 done\n",
      "2860/10000 done\n",
      "2870/10000 done\n",
      "2880/10000 done\n",
      "2890/10000 done\n",
      "2900/10000 done\n",
      "2910/10000 done\n",
      "2920/10000 done\n",
      "2930/10000 done\n",
      "2940/10000 done\n",
      "2950/10000 done\n",
      "2960/10000 done\n",
      "2970/10000 done\n",
      "2980/10000 done\n",
      "2990/10000 done\n",
      "3000/10000 done\n",
      "3010/10000 done\n",
      "3020/10000 done\n",
      "3030/10000 done\n",
      "3040/10000 done\n",
      "3050/10000 done\n",
      "3060/10000 done\n",
      "3070/10000 done\n",
      "3080/10000 done\n",
      "3090/10000 done\n",
      "3100/10000 done\n",
      "3110/10000 done\n",
      "3120/10000 done\n",
      "3130/10000 done\n",
      "3140/10000 done\n",
      "3150/10000 done\n",
      "3160/10000 done\n",
      "3170/10000 done\n",
      "3180/10000 done\n",
      "3190/10000 done\n",
      "3200/10000 done\n",
      "3210/10000 done\n",
      "3220/10000 done\n",
      "3230/10000 done\n",
      "3240/10000 done\n",
      "3250/10000 done\n",
      "3260/10000 done\n",
      "3270/10000 done\n",
      "3280/10000 done\n",
      "3290/10000 done\n",
      "3300/10000 done\n",
      "3310/10000 done\n",
      "3320/10000 done\n",
      "3330/10000 done\n",
      "3340/10000 done\n",
      "3350/10000 done\n",
      "3360/10000 done\n",
      "3370/10000 done\n",
      "3380/10000 done\n",
      "3390/10000 done\n",
      "3400/10000 done\n",
      "3410/10000 done\n",
      "3420/10000 done\n",
      "3430/10000 done\n",
      "3440/10000 done\n",
      "3450/10000 done\n",
      "3460/10000 done\n",
      "3470/10000 done\n",
      "3480/10000 done\n",
      "3490/10000 done\n",
      "3500/10000 done\n",
      "3510/10000 done\n",
      "3520/10000 done\n",
      "3530/10000 done\n",
      "3540/10000 done\n",
      "3550/10000 done\n",
      "3560/10000 done\n",
      "3570/10000 done\n",
      "3580/10000 done\n",
      "3590/10000 done\n",
      "3600/10000 done\n",
      "3610/10000 done\n",
      "3620/10000 done\n",
      "3630/10000 done\n",
      "3640/10000 done\n",
      "3650/10000 done\n",
      "3660/10000 done\n",
      "3670/10000 done\n",
      "3680/10000 done\n",
      "3690/10000 done\n",
      "3700/10000 done\n",
      "3710/10000 done\n",
      "3720/10000 done\n",
      "3730/10000 done\n",
      "3740/10000 done\n",
      "3750/10000 done\n",
      "3760/10000 done\n",
      "3770/10000 done\n",
      "3780/10000 done\n",
      "3790/10000 done\n",
      "3800/10000 done\n",
      "3810/10000 done\n",
      "3820/10000 done\n",
      "3830/10000 done\n",
      "3840/10000 done\n",
      "3850/10000 done\n",
      "3860/10000 done\n",
      "3870/10000 done\n",
      "3880/10000 done\n",
      "3890/10000 done\n",
      "3900/10000 done\n",
      "3910/10000 done\n",
      "3920/10000 done\n",
      "3930/10000 done\n",
      "3940/10000 done\n",
      "3950/10000 done\n",
      "3960/10000 done\n",
      "3970/10000 done\n",
      "3980/10000 done\n",
      "3990/10000 done\n",
      "4000/10000 done\n",
      "4010/10000 done\n",
      "4020/10000 done\n",
      "4030/10000 done\n",
      "4040/10000 done\n",
      "4050/10000 done\n",
      "4060/10000 done\n",
      "4070/10000 done\n",
      "4080/10000 done\n",
      "4090/10000 done\n",
      "4100/10000 done\n",
      "4110/10000 done\n",
      "4120/10000 done\n",
      "4130/10000 done\n",
      "4140/10000 done\n",
      "4150/10000 done\n",
      "4160/10000 done\n",
      "4170/10000 done\n",
      "4180/10000 done\n",
      "4190/10000 done\n",
      "4200/10000 done\n",
      "4210/10000 done\n",
      "4220/10000 done\n",
      "4230/10000 done\n",
      "4240/10000 done\n",
      "4250/10000 done\n",
      "4260/10000 done\n",
      "4270/10000 done\n",
      "4280/10000 done\n",
      "4290/10000 done\n",
      "4300/10000 done\n",
      "4310/10000 done\n",
      "4320/10000 done\n",
      "4330/10000 done\n",
      "4340/10000 done\n",
      "4350/10000 done\n",
      "4360/10000 done\n",
      "4370/10000 done\n",
      "4380/10000 done\n",
      "4390/10000 done\n",
      "4400/10000 done\n",
      "4410/10000 done\n",
      "4420/10000 done\n",
      "4430/10000 done\n",
      "4440/10000 done\n",
      "4450/10000 done\n",
      "4460/10000 done\n",
      "4470/10000 done\n",
      "4480/10000 done\n",
      "4490/10000 done\n",
      "4500/10000 done\n",
      "4510/10000 done\n",
      "4520/10000 done\n",
      "4530/10000 done\n",
      "4540/10000 done\n",
      "4550/10000 done\n",
      "4560/10000 done\n",
      "4570/10000 done\n",
      "4580/10000 done\n",
      "4590/10000 done\n",
      "4600/10000 done\n",
      "4610/10000 done\n",
      "4620/10000 done\n",
      "4630/10000 done\n",
      "4640/10000 done\n",
      "4650/10000 done\n",
      "4660/10000 done\n",
      "4670/10000 done\n",
      "4680/10000 done\n",
      "4690/10000 done\n",
      "4700/10000 done\n",
      "4710/10000 done\n",
      "4720/10000 done\n",
      "4730/10000 done\n",
      "4740/10000 done\n",
      "4750/10000 done\n",
      "4760/10000 done\n",
      "4770/10000 done\n",
      "4780/10000 done\n",
      "4790/10000 done\n",
      "4800/10000 done\n",
      "4810/10000 done\n",
      "4820/10000 done\n",
      "4830/10000 done\n",
      "4840/10000 done\n",
      "4850/10000 done\n",
      "4860/10000 done\n",
      "4870/10000 done\n",
      "4880/10000 done\n",
      "4890/10000 done\n",
      "4900/10000 done\n",
      "4910/10000 done\n",
      "4920/10000 done\n",
      "4930/10000 done\n",
      "4940/10000 done\n",
      "4950/10000 done\n",
      "4960/10000 done\n",
      "4970/10000 done\n",
      "4980/10000 done\n",
      "4990/10000 done\n",
      "5000/10000 done\n",
      "5010/10000 done\n",
      "5020/10000 done\n",
      "5030/10000 done\n",
      "5040/10000 done\n",
      "5050/10000 done\n",
      "5060/10000 done\n",
      "5070/10000 done\n",
      "5080/10000 done\n",
      "5090/10000 done\n",
      "5100/10000 done\n",
      "5110/10000 done\n",
      "5120/10000 done\n",
      "5130/10000 done\n",
      "5140/10000 done\n",
      "5150/10000 done\n",
      "5160/10000 done\n",
      "5170/10000 done\n",
      "5180/10000 done\n",
      "5190/10000 done\n",
      "5200/10000 done\n",
      "5210/10000 done\n",
      "5220/10000 done\n",
      "5230/10000 done\n",
      "5240/10000 done\n",
      "5250/10000 done\n",
      "5260/10000 done\n",
      "5270/10000 done\n",
      "5280/10000 done\n",
      "5290/10000 done\n",
      "5300/10000 done\n",
      "5310/10000 done\n",
      "5320/10000 done\n",
      "5330/10000 done\n",
      "5340/10000 done\n",
      "5350/10000 done\n",
      "5360/10000 done\n",
      "5370/10000 done\n",
      "5380/10000 done\n",
      "5390/10000 done\n",
      "5400/10000 done\n",
      "5410/10000 done\n",
      "5420/10000 done\n",
      "5430/10000 done\n",
      "5440/10000 done\n",
      "5450/10000 done\n",
      "5460/10000 done\n",
      "5470/10000 done\n",
      "5480/10000 done\n",
      "5490/10000 done\n",
      "5500/10000 done\n",
      "5510/10000 done\n",
      "5520/10000 done\n",
      "5530/10000 done\n",
      "5540/10000 done\n",
      "5550/10000 done\n",
      "5560/10000 done\n",
      "5570/10000 done\n",
      "5580/10000 done\n",
      "5590/10000 done\n",
      "5600/10000 done\n",
      "5610/10000 done\n",
      "5620/10000 done\n",
      "5630/10000 done\n",
      "5640/10000 done\n",
      "5650/10000 done\n",
      "5660/10000 done\n",
      "5670/10000 done\n",
      "5680/10000 done\n",
      "5690/10000 done\n",
      "5700/10000 done\n",
      "5710/10000 done\n",
      "5720/10000 done\n",
      "5730/10000 done\n",
      "5740/10000 done\n",
      "5750/10000 done\n",
      "5760/10000 done\n",
      "5770/10000 done\n",
      "5780/10000 done\n",
      "5790/10000 done\n",
      "5800/10000 done\n",
      "5810/10000 done\n",
      "5820/10000 done\n",
      "5830/10000 done\n",
      "5840/10000 done\n",
      "5850/10000 done\n",
      "5860/10000 done\n",
      "5870/10000 done\n",
      "5880/10000 done\n",
      "5890/10000 done\n",
      "5900/10000 done\n",
      "5910/10000 done\n",
      "5920/10000 done\n",
      "5930/10000 done\n",
      "5940/10000 done\n",
      "5950/10000 done\n",
      "5960/10000 done\n",
      "5970/10000 done\n",
      "5980/10000 done\n",
      "5990/10000 done\n",
      "6000/10000 done\n",
      "6010/10000 done\n",
      "6020/10000 done\n",
      "6030/10000 done\n",
      "6040/10000 done\n",
      "6050/10000 done\n",
      "6060/10000 done\n",
      "6070/10000 done\n",
      "6080/10000 done\n",
      "6090/10000 done\n",
      "6100/10000 done\n",
      "6110/10000 done\n",
      "6120/10000 done\n",
      "6130/10000 done\n",
      "6140/10000 done\n",
      "6150/10000 done\n",
      "6160/10000 done\n",
      "6170/10000 done\n",
      "6180/10000 done\n",
      "6190/10000 done\n",
      "6200/10000 done\n",
      "6210/10000 done\n",
      "6220/10000 done\n",
      "6230/10000 done\n",
      "6240/10000 done\n",
      "6250/10000 done\n",
      "6260/10000 done\n",
      "6270/10000 done\n",
      "6280/10000 done\n",
      "6290/10000 done\n",
      "6300/10000 done\n",
      "6310/10000 done\n",
      "6320/10000 done\n",
      "6330/10000 done\n",
      "6340/10000 done\n",
      "6350/10000 done\n",
      "6360/10000 done\n",
      "6370/10000 done\n",
      "6380/10000 done\n",
      "6390/10000 done\n",
      "6400/10000 done\n",
      "6410/10000 done\n",
      "6420/10000 done\n",
      "6430/10000 done\n",
      "6440/10000 done\n",
      "6450/10000 done\n",
      "6460/10000 done\n",
      "6470/10000 done\n",
      "6480/10000 done\n",
      "6490/10000 done\n",
      "6500/10000 done\n",
      "6510/10000 done\n",
      "6520/10000 done\n",
      "6530/10000 done\n",
      "6540/10000 done\n",
      "6550/10000 done\n",
      "6560/10000 done\n",
      "6570/10000 done\n",
      "6580/10000 done\n",
      "6590/10000 done\n",
      "6600/10000 done\n",
      "6610/10000 done\n",
      "6620/10000 done\n",
      "6630/10000 done\n",
      "6640/10000 done\n",
      "6650/10000 done\n",
      "6660/10000 done\n",
      "6670/10000 done\n",
      "6680/10000 done\n",
      "6690/10000 done\n",
      "6700/10000 done\n",
      "6710/10000 done\n",
      "6720/10000 done\n",
      "6730/10000 done\n",
      "6740/10000 done\n",
      "6750/10000 done\n",
      "6760/10000 done\n",
      "6770/10000 done\n",
      "6780/10000 done\n",
      "6790/10000 done\n",
      "6800/10000 done\n",
      "6810/10000 done\n",
      "6820/10000 done\n",
      "6830/10000 done\n",
      "6840/10000 done\n",
      "6850/10000 done\n",
      "6860/10000 done\n",
      "6870/10000 done\n",
      "6880/10000 done\n",
      "6890/10000 done\n",
      "6900/10000 done\n",
      "6910/10000 done\n",
      "6920/10000 done\n",
      "6930/10000 done\n",
      "6940/10000 done\n",
      "6950/10000 done\n",
      "6960/10000 done\n",
      "6970/10000 done\n",
      "6980/10000 done\n",
      "6990/10000 done\n",
      "7000/10000 done\n",
      "7010/10000 done\n",
      "7020/10000 done\n",
      "7030/10000 done\n",
      "7040/10000 done\n",
      "7050/10000 done\n",
      "7060/10000 done\n",
      "7070/10000 done\n",
      "7080/10000 done\n",
      "7090/10000 done\n",
      "7100/10000 done\n",
      "7110/10000 done\n",
      "7120/10000 done\n",
      "7130/10000 done\n",
      "7140/10000 done\n",
      "7150/10000 done\n",
      "7160/10000 done\n",
      "7170/10000 done\n",
      "7180/10000 done\n",
      "7190/10000 done\n",
      "7200/10000 done\n",
      "7210/10000 done\n",
      "7220/10000 done\n",
      "7230/10000 done\n",
      "7240/10000 done\n",
      "7250/10000 done\n",
      "7260/10000 done\n",
      "7270/10000 done\n",
      "7280/10000 done\n",
      "7290/10000 done\n",
      "7300/10000 done\n",
      "7310/10000 done\n",
      "7320/10000 done\n",
      "7330/10000 done\n",
      "7340/10000 done\n",
      "7350/10000 done\n",
      "7360/10000 done\n",
      "7370/10000 done\n",
      "7380/10000 done\n",
      "7390/10000 done\n",
      "7400/10000 done\n",
      "7410/10000 done\n",
      "7420/10000 done\n",
      "7430/10000 done\n",
      "7440/10000 done\n",
      "7450/10000 done\n",
      "7460/10000 done\n",
      "7470/10000 done\n",
      "7480/10000 done\n",
      "7490/10000 done\n",
      "7500/10000 done\n",
      "7510/10000 done\n",
      "7520/10000 done\n",
      "7530/10000 done\n",
      "7540/10000 done\n",
      "7550/10000 done\n",
      "7560/10000 done\n",
      "7570/10000 done\n",
      "7580/10000 done\n",
      "7590/10000 done\n",
      "7600/10000 done\n",
      "7610/10000 done\n",
      "7620/10000 done\n",
      "7630/10000 done\n",
      "7640/10000 done\n",
      "7650/10000 done\n",
      "7660/10000 done\n",
      "7670/10000 done\n",
      "7680/10000 done\n",
      "7690/10000 done\n",
      "7700/10000 done\n",
      "7710/10000 done\n",
      "7720/10000 done\n",
      "7730/10000 done\n",
      "7740/10000 done\n",
      "7750/10000 done\n",
      "7760/10000 done\n",
      "7770/10000 done\n",
      "7780/10000 done\n",
      "7790/10000 done\n",
      "7800/10000 done\n",
      "7810/10000 done\n",
      "7820/10000 done\n",
      "7830/10000 done\n",
      "7840/10000 done\n",
      "7850/10000 done\n",
      "7860/10000 done\n",
      "7870/10000 done\n",
      "7880/10000 done\n",
      "7890/10000 done\n",
      "7900/10000 done\n",
      "7910/10000 done\n",
      "7920/10000 done\n",
      "7930/10000 done\n",
      "7940/10000 done\n",
      "7950/10000 done\n",
      "7960/10000 done\n",
      "7970/10000 done\n",
      "7980/10000 done\n",
      "7990/10000 done\n",
      "8000/10000 done\n",
      "8010/10000 done\n",
      "8020/10000 done\n",
      "8030/10000 done\n",
      "8040/10000 done\n",
      "8050/10000 done\n",
      "8060/10000 done\n",
      "8070/10000 done\n",
      "8080/10000 done\n",
      "8090/10000 done\n",
      "8100/10000 done\n",
      "8110/10000 done\n",
      "8120/10000 done\n",
      "8130/10000 done\n",
      "8140/10000 done\n",
      "8150/10000 done\n",
      "8160/10000 done\n",
      "8170/10000 done\n",
      "8180/10000 done\n",
      "8190/10000 done\n",
      "8200/10000 done\n",
      "8210/10000 done\n",
      "8220/10000 done\n",
      "8230/10000 done\n",
      "8240/10000 done\n",
      "8250/10000 done\n",
      "8260/10000 done\n",
      "8270/10000 done\n",
      "8280/10000 done\n",
      "8290/10000 done\n",
      "8300/10000 done\n",
      "8310/10000 done\n",
      "8320/10000 done\n",
      "8330/10000 done\n",
      "8340/10000 done\n",
      "8350/10000 done\n",
      "8360/10000 done\n",
      "8370/10000 done\n",
      "8380/10000 done\n",
      "8390/10000 done\n",
      "8400/10000 done\n",
      "8410/10000 done\n",
      "8420/10000 done\n",
      "8430/10000 done\n",
      "8440/10000 done\n",
      "8450/10000 done\n",
      "8460/10000 done\n",
      "8470/10000 done\n",
      "8480/10000 done\n",
      "8490/10000 done\n",
      "8500/10000 done\n",
      "8510/10000 done\n",
      "8520/10000 done\n",
      "8530/10000 done\n",
      "8540/10000 done\n",
      "8550/10000 done\n",
      "8560/10000 done\n",
      "8570/10000 done\n",
      "8580/10000 done\n",
      "8590/10000 done\n",
      "8600/10000 done\n",
      "8610/10000 done\n",
      "8620/10000 done\n",
      "8630/10000 done\n",
      "8640/10000 done\n",
      "8650/10000 done\n",
      "8660/10000 done\n",
      "8670/10000 done\n",
      "8680/10000 done\n",
      "8690/10000 done\n",
      "8700/10000 done\n",
      "8710/10000 done\n",
      "8720/10000 done\n",
      "8730/10000 done\n",
      "8740/10000 done\n",
      "8750/10000 done\n",
      "8760/10000 done\n",
      "8770/10000 done\n",
      "8780/10000 done\n",
      "8790/10000 done\n",
      "8800/10000 done\n",
      "8810/10000 done\n",
      "8820/10000 done\n",
      "8830/10000 done\n",
      "8840/10000 done\n",
      "8850/10000 done\n",
      "8860/10000 done\n",
      "8870/10000 done\n",
      "8880/10000 done\n",
      "8890/10000 done\n",
      "8900/10000 done\n",
      "8910/10000 done\n",
      "8920/10000 done\n",
      "8930/10000 done\n",
      "8940/10000 done\n",
      "8950/10000 done\n",
      "8960/10000 done\n",
      "8970/10000 done\n",
      "8980/10000 done\n",
      "8990/10000 done\n",
      "9000/10000 done\n",
      "9010/10000 done\n",
      "9020/10000 done\n",
      "9030/10000 done\n",
      "9040/10000 done\n",
      "9050/10000 done\n",
      "9060/10000 done\n",
      "9070/10000 done\n",
      "9080/10000 done\n",
      "9090/10000 done\n",
      "9100/10000 done\n",
      "9110/10000 done\n",
      "9120/10000 done\n",
      "9130/10000 done\n",
      "9140/10000 done\n",
      "9150/10000 done\n",
      "9160/10000 done\n",
      "9170/10000 done\n",
      "9180/10000 done\n",
      "9190/10000 done\n",
      "9200/10000 done\n",
      "9210/10000 done\n",
      "9220/10000 done\n",
      "9230/10000 done\n",
      "9240/10000 done\n",
      "9250/10000 done\n",
      "9260/10000 done\n",
      "9270/10000 done\n",
      "9280/10000 done\n",
      "9290/10000 done\n",
      "9300/10000 done\n",
      "9310/10000 done\n",
      "9320/10000 done\n",
      "9330/10000 done\n",
      "9340/10000 done\n",
      "9350/10000 done\n",
      "9360/10000 done\n",
      "9370/10000 done\n",
      "9380/10000 done\n",
      "9390/10000 done\n",
      "9400/10000 done\n",
      "9410/10000 done\n",
      "9420/10000 done\n",
      "9430/10000 done\n",
      "9440/10000 done\n",
      "9450/10000 done\n",
      "9460/10000 done\n",
      "9470/10000 done\n",
      "9480/10000 done\n",
      "9490/10000 done\n",
      "9500/10000 done\n",
      "9510/10000 done\n",
      "9520/10000 done\n",
      "9530/10000 done\n",
      "9540/10000 done\n",
      "9550/10000 done\n",
      "9560/10000 done\n",
      "9570/10000 done\n",
      "9580/10000 done\n",
      "9590/10000 done\n",
      "9600/10000 done\n",
      "9610/10000 done\n",
      "9620/10000 done\n",
      "9630/10000 done\n",
      "9640/10000 done\n",
      "9650/10000 done\n",
      "9660/10000 done\n",
      "9670/10000 done\n",
      "9680/10000 done\n",
      "9690/10000 done\n",
      "9700/10000 done\n",
      "9710/10000 done\n",
      "9720/10000 done\n",
      "9730/10000 done\n",
      "9740/10000 done\n",
      "9750/10000 done\n",
      "9760/10000 done\n",
      "9770/10000 done\n",
      "9780/10000 done\n",
      "9790/10000 done\n",
      "9800/10000 done\n",
      "9810/10000 done\n",
      "9820/10000 done\n",
      "9830/10000 done\n",
      "9840/10000 done\n",
      "9850/10000 done\n",
      "9860/10000 done\n",
      "9870/10000 done\n",
      "9880/10000 done\n",
      "9890/10000 done\n",
      "9900/10000 done\n",
      "9910/10000 done\n",
      "9920/10000 done\n",
      "9930/10000 done\n",
      "9940/10000 done\n",
      "9950/10000 done\n",
      "9960/10000 done\n",
      "9970/10000 done\n",
      "9980/10000 done\n",
      "9990/10000 done\n",
      "10000/10000 done\n"
     ]
    }
   ],
   "source": [
    "def projection(matrix):\n",
    "    return np.real(np.matrix([[matrix[0,0],0,0,matrix[0,3]/2+matrix[3,0]/2],\\\n",
    "                                [0,matrix[2,2]/2+matrix[1,1]/2,0,0],\\\n",
    "                                [0,0,matrix[2,2]/2+matrix[1,1]/2,0],\\\n",
    "                                [matrix[0,3]/2+matrix[3,0]/2,0,0,matrix[3,3]]]))\n",
    "samples = []\n",
    "for i in range(10000):\n",
    "    initial_matrix = rand_PSDM(4)\n",
    "    projected = projection(initial_matrix)\n",
    "    parameters = [projected[0,0], projected[0,3], projected[1,1], projected[3,3]]\n",
    "    projected = density_matrix(projected)\n",
    "    projected.set(20000)\n",
    "    samples.append((projected.data, parameters))\n",
    "    if (i+1)%10 == 0:\n",
    "        print(f'{i+1}/10000 done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_new = []\n",
    "for hist, par in samples_load:\n",
    "    samples_new.append((data_to_bins(hist), par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ML_proj_samps.dat', 'wb') as file:\n",
    "    pickle.dump(samples_new, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ML_proj_samps.dat', 'rb') as file:\n",
    "    samples_load = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_train = samples_new[:8000]\n",
    "samples_val = samples_new[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24111, 0.20476, 0.2126 , ..., 0.24117, 0.20626, 0.24237])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist, params = samples_load[0]\n",
    "hist.astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class proj_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,samples):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.samples = samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hist, params = self.samples[idx]\n",
    "        hist = torch.from_numpy(hist).float()\n",
    "        params = torch.Tensor(params).float()\n",
    "        return hist, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input=100, output=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input, 50)\n",
    "        self.output_layer = nn.Linear(20, output)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(50, 100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(100,50),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(50,70),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(70,20),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.3),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "        self.weights=torch.Tensor([0.1,0.1,0.7,0.1])\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "            loss = (inputs - targets) * (inputs - targets) @ self.weights\n",
    "            return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeError(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RelativeError, self).__init__()\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        loss = torch.abs((inputs - targets) * (inputs - targets) / (inputs - targets.mean()))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainig & validation loop\n",
    "def fit(model, samples_train, samples_val, batch_size=10, lr=0.05, epochs=10, dataset = proj_dataset, criterion=nn.MSELoss()):    \n",
    "    from torchmetrics import MeanSquaredError\n",
    "    data_loader_train = DataLoader(dataset(samples_train), shuffle=True, batch_size=batch_size)\n",
    "    data_loader_val = DataLoader(dataset(samples_val), shuffle=True, batch_size=batch_size)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr) \n",
    "    metrics = MeanSquaredError()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #Training\n",
    "        model.train()\n",
    "        for hist, params in data_loader_train:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(hist)\n",
    "            loss = criterion(pred, params)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            metrics(pred, params)\n",
    "        train_loss = metrics.compute()\n",
    "        metrics.reset()\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for hist, params in data_loader_val:\n",
    "                pred = model(hist)\n",
    "                metrics(pred, params)\n",
    "        val_loss = metrics.compute()\n",
    "        metrics.reset()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: training loss: {train_loss:.5f}, validation loss: {val_loss:.5f}')\n",
    "    \n",
    "    return model    \n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, samples_eval, batch_size=10, dataset=proj_dataset):\n",
    "    import torchmetrics\n",
    "    metrics = torchmetrics.MeanSquaredError()\n",
    "    data_loader = DataLoader(dataset(samples_eval), shuffle=True, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for hist, params in data_loader:\n",
    "            pred = model(hist)\n",
    "            metrics(pred, params)\n",
    "    return metrics.compute()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: training loss: 6.16205, validation loss: 3.84761\n",
      "Epoch 2/10: training loss: 4.76951, validation loss: 2.31848\n",
      "Epoch 3/10: training loss: 9.07952, validation loss: 10.34978\n",
      "Epoch 4/10: training loss: 13.53448, validation loss: 8.06730\n",
      "Epoch 5/10: training loss: 10.90095, validation loss: 5.24700\n",
      "Epoch 6/10: training loss: 10.13042, validation loss: 2.01788\n",
      "Epoch 7/10: training loss: 9.83839, validation loss: 1.30464\n",
      "Epoch 8/10: training loss: 9.08703, validation loss: 0.83131\n",
      "Epoch 9/10: training loss: 7.90574, validation loss: 0.43679\n",
      "Epoch 10/10: training loss: 8.53169, validation loss: 0.79762\n"
     ]
    }
   ],
   "source": [
    "#from torchmetrics.functional.regression import relative_squared_error\n",
    "model = fit(Net(), samples_train, samples_val, batch_size=50, lr=0.05, epochs=10, criterion=RelativeError())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: training loss: 0.04803, validation loss: 0.01149\n",
      "Epoch 2/10: training loss: 0.01209, validation loss: 0.00853\n",
      "Epoch 3/10: training loss: 0.00932, validation loss: 0.00913\n",
      "Epoch 4/10: training loss: 0.01676, validation loss: 0.00974\n",
      "Epoch 5/10: training loss: 0.01048, validation loss: 0.01001\n",
      "Epoch 6/10: training loss: 0.01023, validation loss: 0.00948\n",
      "Epoch 7/10: training loss: 0.00973, validation loss: 0.01008\n",
      "Epoch 8/10: training loss: 0.00968, validation loss: 0.00918\n",
      "Epoch 9/10: training loss: 0.01137, validation loss: 0.01011\n",
      "Epoch 10/10: training loss: 0.00991, validation loss: 0.00985\n"
     ]
    }
   ],
   "source": [
    "model2 = fit(Net(output=4), samples_train, samples_val, batch_size=10, lr=0.05, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'proj_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2302, -0.0687,  0.2423,  0.2648], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.eval()\n",
    "model2(torch.from_numpy(samples_new[6][0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1226,  0.7979,  0.2249, -1.2880], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model(torch.from_numpy(samples_new[45][0]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = Net()\n",
    "model_loaded.load_state_dict(torch.load('proj_model.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Werner states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angle</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>[0.0, 0.01]</th>\n",
       "      <th>[0.01, 0.02]</th>\n",
       "      <th>[0.02, 0.03]</th>\n",
       "      <th>[0.03, 0.04]</th>\n",
       "      <th>[0.04, 0.05]</th>\n",
       "      <th>[0.05, 0.06]</th>\n",
       "      <th>[0.06, 0.07]</th>\n",
       "      <th>[0.07, 0.08]</th>\n",
       "      <th>...</th>\n",
       "      <th>[0.9, 0.91]</th>\n",
       "      <th>[0.91, 0.92]</th>\n",
       "      <th>[0.92, 0.93]</th>\n",
       "      <th>[0.93, 0.94]</th>\n",
       "      <th>[0.94, 0.95]</th>\n",
       "      <th>[0.95, 0.96]</th>\n",
       "      <th>[0.96, 0.97]</th>\n",
       "      <th>[0.97, 0.98]</th>\n",
       "      <th>[0.98, 0.99]</th>\n",
       "      <th>[0.99, 1.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333026</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.568860</td>\n",
       "      <td>0.448408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.620751</td>\n",
       "      <td>0.267946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.635826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.401182</td>\n",
       "      <td>0.933977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00898</td>\n",
       "      <td>0.02606</td>\n",
       "      <td>0.02698</td>\n",
       "      <td>0.02618</td>\n",
       "      <td>0.02702</td>\n",
       "      <td>0.02636</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Angle  Visibility  [0.0, 0.01]  [0.01, 0.02]  [0.02, 0.03]  \\\n",
       "0  0.333026    0.045424          0.0       0.00000       0.00000   \n",
       "1  0.568860    0.448408          0.0       0.00000       0.00000   \n",
       "2  0.620751    0.267946          0.0       0.00000       0.00000   \n",
       "3  0.127500    0.635826          0.0       0.00000       0.00000   \n",
       "4  0.401182    0.933977          0.0       0.00898       0.02606   \n",
       "\n",
       "   [0.03, 0.04]  [0.04, 0.05]  [0.05, 0.06]  [0.06, 0.07]  [0.07, 0.08]  ...  \\\n",
       "0       0.00000       0.00000       0.00000       0.00000        0.0000  ...   \n",
       "1       0.00000       0.00000       0.00000       0.00000        0.0000  ...   \n",
       "2       0.00000       0.00000       0.00000       0.00000        0.0000  ...   \n",
       "3       0.00000       0.00000       0.00000       0.00000        0.0000  ...   \n",
       "4       0.02698       0.02618       0.02702       0.02636        0.0261  ...   \n",
       "\n",
       "   [0.9, 0.91]  [0.91, 0.92]  [0.92, 0.93]  [0.93, 0.94]  [0.94, 0.95]  \\\n",
       "0          0.0           0.0           0.0           0.0           0.0   \n",
       "1          0.0           0.0           0.0           0.0           0.0   \n",
       "2          0.0           0.0           0.0           0.0           0.0   \n",
       "3          0.0           0.0           0.0           0.0           0.0   \n",
       "4          0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   [0.95, 0.96]  [0.96, 0.97]  [0.97, 0.98]  [0.98, 0.99]  [0.99, 1.0]  \n",
       "0           0.0           0.0           0.0           0.0          0.0  \n",
       "1           0.0           0.0           0.0           0.0          0.0  \n",
       "2           0.0           0.0           0.0           0.0          0.0  \n",
       "3           0.0           0.0           0.0           0.0          0.0  \n",
       "4           0.0           0.0           0.0           0.0          0.0  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('werner_sample.csv', index_col='Unnamed: 0')\n",
    "df_test = pd.read_csv('werner_test.csv', index_col='Unnamed: 0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_wer_data(df):\n",
    "    wer_data=[]\n",
    "    for i in range(len(df)):\n",
    "        wer_data.append((df.iloc[i].values[2:], df.iloc[i].values[:2]))\n",
    "    return wer_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_data = df_to_wer_data(df)\n",
    "wer_test = df_to_wer_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class werner_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, transform = torch.from_numpy):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        hist, params = self.data[idx]\n",
    "        hist = self.transform(hist).float()\n",
    "        params = self.transform(params).float()\n",
    "        return hist, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input=100, output=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input, 95)\n",
    "        self.output_layer = nn.Linear(95, output)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(95, 95),\n",
    "            nn.Sigmoid(),\n",
    "            ##nn.Dropout(p=0.5),\n",
    "            nn.Linear(95,95),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: training loss: 1.57334, validation loss: 1.63343\n",
      "Epoch 2/10: training loss: 1.14651, validation loss: 0.29367\n",
      "Epoch 3/10: training loss: 1.18699, validation loss: 2.72770\n",
      "Epoch 4/10: training loss: 1.16681, validation loss: 1.50514\n",
      "Epoch 5/10: training loss: 1.17457, validation loss: 1.30213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_wer \u001b[38;5;241m=\u001b[39m fit(WerNet(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), wer_data[:\u001b[38;5;241m8000\u001b[39m], wer_data[\u001b[38;5;241m8000\u001b[39m:], dataset \u001b[38;5;241m=\u001b[39m werner_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, samples_train, samples_val, batch_size, lr, epochs, dataset, criterion)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hist, params \u001b[38;5;129;01min\u001b[39;00m data_loader_train:\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(hist)\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, params)\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m, in \u001b[0;36mWerNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(x)\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers(x)\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(x)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_wer = fit(WerNet(input=100, output=2), wer_data[:8000], wer_data[8000:], dataset = werner_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_wer.state_dict(), 'model_wer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = WerNet(input=100, output=2)\n",
    "model_loaded.load_state_dict(torch.load('model_wer.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0018)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('werner_test.csv', index_col='Unnamed: 0')\n",
    "wer_test = df_to_wer_data(df_test)\n",
    "evaluate_model(model_loaded, wer_test, dataset=werner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.StepPatch at 0x13c319c0090>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "max_err = 0\n",
    "worst_hist=None\n",
    "worst_params=None\n",
    "model_loaded.eval()\n",
    "metrics = torchmetrics.MeanSquaredError() \n",
    "for hist, params in DataLoader(werner_dataset(wer_test), batch_size=1):\n",
    "    pred = model_loaded(hist)\n",
    "    err = metrics(pred, params)\n",
    "    if err > max_err:\n",
    "        max_err = err\n",
    "        worst_hist = hist\n",
    "        worst_pred = pred\n",
    "        worst_params = params\n",
    "\n",
    "alp, vis = worst_pred.detach().numpy()[0]\n",
    "worst_dm = density_matrix(rho2(alp, vis))\n",
    "plt.stairs(worst_hist.numpy()[0], fill=True)\n",
    "plt.stairs(worst_dm.bins()['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.StepPatch at 0x13c31bb5310>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchmetrics\n",
    "min_err = 80\n",
    "best_hist=None\n",
    "best_params=None\n",
    "model_loaded.eval()\n",
    "metrics = torchmetrics.MeanSquaredError() \n",
    "for hist, params in DataLoader(werner_dataset(wer_test), batch_size=1):\n",
    "    pred = model_loaded(hist)\n",
    "    err = metrics(pred, params)\n",
    "    if err < min_err:\n",
    "        min_err = err\n",
    "        best_hist = hist\n",
    "        best_pred = pred\n",
    "        best_params = params\n",
    "\n",
    "alp, vis = best_pred.detach().numpy()[0]\n",
    "best_dm = density_matrix(rho2(alp, vis))\n",
    "plt.stairs(best_hist.numpy()[0], fill=True)\n",
    "plt.stairs(best_dm.bins()['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
